const items = [
  {
    jobTitle: 'Data Engineer Intern',
    company: 'Netflix',
    companyUrl: 'https://www.netflix.com',
    companyLogo: '/static/images/Company/Netflix.png',
    startDate: '2025-06-16',
    endDate: '2025-09-05',
    location: 'Los Gatos, California',
    roleType: 'Internship',
    technologies: [
      'Airflow',
      'Snowflake',
      'Kafka',
      'Hive',
      'Spark',
      'Python',
      'Trino',
      'AWS',
      'Iceberg',
      'Maestro',
      'Dataflow',
    ],
    highlights: [
      'Attributed $17M annualized infrastructure Kafka cost, providing cost visibility for ~500K topics',
      'Enriched ownership achieving >97% ownership match rate',
    ],
    description: [
      "Architected and productionized an end-to-end ETL pipeline to attribute $17M annualized infrastructure cost for Netflix's Kafka-as-a-Service (KaaS), providing topic-level cost visibility for ~500K topics and attributing >98% of cluster costs",
      'Built a weighted Spark-based cost allocation model that apportioned cluster costs using proportional consumption signals (disk, network, vCores, partition count) and validated outputs against billing datasets.',
      'Integrated ownership enrichment by joining Kafka inventory with the Netflix Data Catalog to deliver >97% ownership match rate, enabling chargeback and governance workflows.',
      "Implemented a repository-parsing SQL Query Generator that interpreted ~200 foundational schemas to autonomously produce business-relevant SQL queries, cutting analysts' manual query time by ~40 hours/week.",
      'Strengthened pipeline reliability by adding targeted data quality audits and monitoring, directly contributing to a 70% reduction in on-call failures.',
    ],
  },
  {
    jobTitle: 'Data Engineer Co-op',
    company: 'Glassdoor',
    companyUrl: 'https://glassdoor.com',
    companyLogo: '/static/images/Company/Glassdoor.png',
    startDate: '2024-08-01',
    endDate: '2024-12-31',
    location: 'San Francisco, California',
    roleType: 'Co-op',
    technologies: [
      'Snowflake',
      'Airflow',
      'Kafka',
      'Hive',
      'Apache Iceberg',
      'Slack',
      'PagerDuty',
    ],
    highlights: [
      'Reduced operational costs by $1.1M',
      'Achieved 99.7% data accuracy on 10TB pipeline',
    ],
    description: [
      'Spearheaded the migration of the AppsFlyer data pipeline from API-based extraction to a Snowflake-based solution, <b>slashing data latency by 95%</b> and saving $1.1M in operational costs.',
      'Designed and implemented Airflow DAGs for transferring 10TB+ data from Snowflake to Hive, <b>achieving 99.7% data accuracy</b> and a 40% boost in processing speed.',
      'Integrated Apache Iceberg to optimize storage and improve query execution times by 35%.',
      'Built cost-monitoring dashboards in Snowflake, <i>reducing monthly spend by 18%</i>.',
      'Automated SLA monitoring with Slack/PagerDuty alerts, <i>reducing SLA violations by 60%</i>.',
    ],
  },
  {
    jobTitle: 'Data Engineer Intern',
    company: 'Glassdoor',
    companyUrl: 'https://glassdoor.com',
    companyLogo: '/static/images/Company/Glassdoor.png',
    startDate: '2024-06-01',
    endDate: '2024-08-01',
    location: 'San Francisco, California',
    roleType: 'Internship',
    technologies: ['Apache Kafka', 'Confluent', 'Snowflake', 'KSQL', 'S3'],
    highlights: [
      'Improved Kafka replication by 30%',
      'Ensured 99% data consistency through real-time monitoring',
    ],
    description: [
      'Developed the POC for AppsFlyer data migration to Snowflake, enabling full-scale implementation.',
      'Configured Confluent Kafka clusters to mirror production-staging environments, <b>improving replication by 30%</b>.',
      'Built Kafka streaming pipelines to ingest data from S3 to Snowflake, applying real-time transformations using KSQL Streams.',
      'Implemented real-time error handling and monitoring mechanisms to <b>ensure 99% data consistency</b>.',
    ],
  },
  {
    jobTitle: 'Data Engineering Research Assistant',
    company: 'San Francisco State University',
    companyLogo: '/static/images/Company/SFSU.png',
    companyUrl: 'https://www.sfsu.edu/index.html',
    startDate: '2023-09-06',
    endDate: '2024-03-15',
    location: 'San Francisco, California',
    roleType: 'Research',
    technologies: ['Apache Spark', 'Python', 'OpenAI', 'Algolia'],
    highlights: [
      'Reduced processing time by 40%',
      'Achieved sub-100ms Algolia response time',
    ],
    description: [
      'Migrated Python ML models to Spark, <b>improving computation speed by 40%</b>.',
      'Automated Algolia search index creation, <b>achieving sub-100ms search response time</b>.',
      'Developed OpenAI-powered chatbot trained on FDA drug data, <i>increasing user engagement and safety</i>.',
      'Published internal documentation for chatbot performance tuning and prompt optimization.',
    ],
  },
  {
    jobTitle: 'Graduate Assistant',
    company: 'San Francisco State University',
    companyLogo: '/static/images/Company/SFSU.png',
    companyUrl: 'https://www.sfsu.edu/index.html',
    startDate: '2023-09-25',
    endDate: '2024-05-30',
    location: 'San Francisco, California',
    roleType: 'Part-time',
    technologies: ['Qualtrics', 'Microsoft Excel', 'Python'],
    highlights: ['Improved survey data quality by 30%'],
    description: [
      'Streamlined survey data collection pipeline in the School of Nursing using Python and Excel.',
      'Enhanced data quality by 30% and reduced analysis preparation time by half.',
      'Created automated dashboards for institutional decision-making.',
    ],
  },
  {
    jobTitle: 'Data Engineer',
    company: 'Accenture Strategy & Consulting',
    companyUrl: 'https://www.accenture.com/us-en/about/consulting-index',
    companyLogo: '/static/images/Company/Accenture.png',
    startDate: '2022-07-01',
    endDate: '2023-07-21',
    location: 'Gujarat, India',
    roleType: 'Full-time',
    technologies: [
      'GCP',
      'AWS',
      'Redshift',
      'RDS',
      'Airflow',
      'Helm',
      'Kubernetes',
      'Spark',
      'Python',
      'SQL',
    ],
    highlights: [
      'Reduced DB update time by 98.33%',
      'Improved dashboard performance by 30%',
      'Reduced operational costs by 25%',
    ],
    description: [
      'Led migration of 39 AWS-backed Tableau dashboards to GCP, <b>improving performance by 30%</b> and <b>cutting costs by 25%</b>.',
      'Developed Python scripts for RDS and Redshift schema migrations, <b>reducing DB update time by 98.33%</b>.',
      'Created an LLM-powered drag-and-drop interface to auto-generate Apache Airflow DAGs using natural language.',
      'Deployed scalable DAG management solutions using Helm and Kubernetes across MWAA and GCP Composer.',
      'Mentored 3 junior engineers and onboarded them to the teamâ€™s data orchestration framework.',
    ],
  },
  {
    jobTitle: 'Data Engineer Intern',
    company: 'Accenture Strategy & Consulting',
    companyUrl: 'https://www.accenture.com/us-en/about/consulting-index',
    companyLogo: '/static/images/Company/Accenture.png',
    startDate: '2022-01-07',
    endDate: '2022-06-21',
    location: 'Gujarat, India',
    roleType: 'Internship',
    technologies: ['Scala', 'Apache Spark', 'AWS Lambda', 'SNS'],
    highlights: [
      'Cut migration time by 40%',
      'Improved data security with encryption module',
    ],
    description: [
      'Built encryption and hashing modules using Scala and Apache Spark to <b>improve data security</b> for an enterprise ETL platform.',
      'Developed AWS Lambda functions integrated with SNS for <i>automating migration workflows</i>.',
      'Improved ETL pipeline reliability and adherence to KPIs by automating data validation.',
    ],
  },
  {
    jobTitle: 'Data Engineer Intern',
    company: 'Square (HOPS Healthcare)',
    companyUrl: 'https://hops.healthcare/',
    companyLogo: '/static/images/Company/HopsHealthcare.png',
    startDate: '2021-03-01',
    endDate: '2021-06-30',
    location: 'Gujarat, India',
    roleType: 'Internship',
    technologies: [
      'Apache Kafka',
      'Apache Flink',
      'Apache Spark',
      'BioBERT',
      'Django',
      'MongoDB',
      'Python',
    ],
    highlights: [
      'Enabled fraud detection on 10M+ transactions daily',
      'Boosted ETL performance by 35%',
    ],
    description: [
      'Developed a real-time ingestion pipeline using Kafka and Flink for <b>fraud detection across 10M+ daily transactions</b>.',
      'Launched Django web app for secure healthcare report storage, <i>improving access latency by 40%</i>.',
      'Built a custom Spark optimizer to adjust shuffle partitions, <b>improving ETL performance by 35%</b>.',
      'Engineered a BioBERT + Regex-powered parsing bot for extracting structured data from clinical reports.',
    ],
  },
]

export default items
