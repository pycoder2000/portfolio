const items = [
  {
    jobTitle: 'Data Engineer Intern',
    company: 'Netflix',
    companyUrl: 'https://www.netflix.com',
    companyLogo: '/static/images/Company/Netflix.png',
    startDate: '2025-05-19',
    endDate: '2025-08-06',
    location: 'Los Gatos, California',
    roleType: 'Internship',
    technologies: [
      'Airflow',
      'Snowflake',
      'Kafka',
      'Hive',
      'Spark',
      'Python',
      'Netflix Metaflow',
      'Trino',
      'AWS',
    ],
    highlights: [],
    description: [
      'Incoming <b>Data Engineering Intern at Netflix</b>, working on scalable data pipelines and product analytics within the Studio Data Science & Engineering team.',
      'Focus areas: <i>building robust batch/streaming pipelines</i>, optimizing data transformations, and supporting experimentation platforms at scale.',
    ],
  },
  {
    jobTitle: 'Data Engineer Co-op',
    company: 'Glassdoor',
    companyUrl: 'https://glassdoor.com',
    companyLogo: '/static/images/Company/Glassdoor.png',
    startDate: '2024-08-01',
    endDate: '2024-12-31',
    location: 'San Francisco, California',
    roleType: 'Co-op',
    technologies: [
      'Snowflake',
      'Airflow',
      'Kafka',
      'Hive',
      'Apache Iceberg',
      'Slack',
      'PagerDuty',
    ],
    highlights: [
      'Reduced operational costs by $1.1M',
      'Achieved 99.7% data accuracy on 10TB pipeline',
    ],
    description: [
      'Spearheaded the migration of the AppsFlyer data pipeline from API-based extraction to a Snowflake-based solution, <b>slashing data latency by 95%</b> and saving $1.1M in operational costs.',
      'Designed and implemented Airflow DAGs for transferring 10TB+ data from Snowflake to Hive, <b>achieving 99.7% data accuracy</b> and a 40% boost in processing speed.',
      'Integrated Apache Iceberg to optimize storage and improve query execution times by 35%.',
      'Built cost-monitoring dashboards in Snowflake, <i>reducing monthly spend by 18%</i>.',
      'Automated SLA monitoring with Slack/PagerDuty alerts, <i>reducing SLA violations by 60%</i>.',
    ],
  },
  {
    jobTitle: 'Data Engineer Intern',
    company: 'Glassdoor',
    companyUrl: 'https://glassdoor.com',
    companyLogo: '/static/images/Company/Glassdoor.png',
    startDate: '2024-06-01',
    endDate: '2024-08-01',
    location: 'San Francisco, California',
    roleType: 'Internship',
    technologies: ['Apache Kafka', 'Confluent', 'Snowflake', 'KSQL', 'S3'],
    highlights: [
      'Improved Kafka replication by 30%',
      'Ensured 99% data consistency through real-time monitoring',
    ],
    description: [
      'Developed the POC for AppsFlyer data migration to Snowflake, enabling full-scale implementation.',
      'Configured Confluent Kafka clusters to mirror production-staging environments, <b>improving replication by 30%</b>.',
      'Built Kafka streaming pipelines to ingest data from S3 to Snowflake, applying real-time transformations using KSQL Streams.',
      'Implemented real-time error handling and monitoring mechanisms to <b>ensure 99% data consistency</b>.',
    ],
  },
  {
    jobTitle: 'Data Engineering Research Assistant',
    company: 'San Francisco State University',
    companyLogo: '/static/images/Company/SFSU.png',
    companyUrl: 'https://www.sfsu.edu/index.html',
    startDate: '2023-09-06',
    endDate: '2024-03-15',
    location: 'San Francisco, California',
    roleType: 'Research',
    technologies: ['Apache Spark', 'Python', 'OpenAI', 'Algolia'],
    highlights: [
      'Reduced processing time by 40%',
      'Achieved sub-100ms Algolia response time',
    ],
    description: [
      'Migrated Python ML models to Spark, <b>improving computation speed by 40%</b>.',
      'Automated Algolia search index creation, <b>achieving sub-100ms search response time</b>.',
      'Developed OpenAI-powered chatbot trained on FDA drug data, <i>increasing user engagement and safety</i>.',
      'Published internal documentation for chatbot performance tuning and prompt optimization.',
    ],
  },
  {
    jobTitle: 'Graduate Assistant',
    company: 'San Francisco State University',
    companyLogo: '/static/images/Company/SFSU.png',
    companyUrl: 'https://www.sfsu.edu/index.html',
    startDate: '2023-09-25',
    endDate: '2024-05-30',
    location: 'San Francisco, California',
    roleType: 'Part-time',
    technologies: ['Qualtrics', 'Microsoft Excel', 'Python'],
    highlights: ['Improved survey data quality by 30%'],
    description: [
      'Streamlined survey data collection pipeline in the School of Nursing using Python and Excel.',
      'Enhanced data quality by 30% and reduced analysis preparation time by half.',
      'Created automated dashboards for institutional decision-making.',
    ],
  },
  {
    jobTitle: 'Data Engineer',
    company: 'Accenture Strategy & Consulting',
    companyUrl: 'https://www.accenture.com/us-en/about/consulting-index',
    companyLogo: '/static/images/Company/Accenture.png',
    startDate: '2022-07-01',
    endDate: '2023-07-21',
    location: 'Gujarat, India',
    roleType: 'Full-time',
    technologies: [
      'GCP',
      'AWS',
      'Redshift',
      'RDS',
      'Airflow',
      'Helm',
      'Kubernetes',
      'Spark',
      'Python',
      'SQL',
    ],
    highlights: [
      'Reduced DB update time by 98.33%',
      'Improved dashboard performance by 30%',
      'Reduced operational costs by 25%',
    ],
    description: [
      'Led migration of 39 AWS-backed Tableau dashboards to GCP, <b>improving performance by 30%</b> and <b>cutting costs by 25%</b>.',
      'Developed Python scripts for RDS and Redshift schema migrations, <b>reducing DB update time by 98.33%</b>.',
      'Created an LLM-powered drag-and-drop interface to auto-generate Apache Airflow DAGs using natural language.',
      'Deployed scalable DAG management solutions using Helm and Kubernetes across MWAA and GCP Composer.',
      'Mentored 3 junior engineers and onboarded them to the teamâ€™s data orchestration framework.',
    ],
  },
  {
    jobTitle: 'Data Engineer Intern',
    company: 'Accenture Strategy & Consulting',
    companyUrl: 'https://www.accenture.com/us-en/about/consulting-index',
    companyLogo: '/static/images/Company/Accenture.png',
    startDate: '2022-01-07',
    endDate: '2022-06-21',
    location: 'Gujarat, India',
    roleType: 'Internship',
    technologies: ['Scala', 'Apache Spark', 'AWS Lambda', 'SNS'],
    highlights: [
      'Cut migration time by 40%',
      'Improved data security with encryption module',
    ],
    description: [
      'Built encryption and hashing modules using Scala and Apache Spark to <b>improve data security</b> for an enterprise ETL platform.',
      'Developed AWS Lambda functions integrated with SNS for <i>automating migration workflows</i>.',
      'Improved ETL pipeline reliability and adherence to KPIs by automating data validation.',
    ],
  },
  {
    jobTitle: 'Data Engineer Intern',
    company: 'Square (HOPS Healthcare)',
    companyUrl: 'https://hops.healthcare/',
    companyLogo: '/static/images/Company/HopsHealthcare.png',
    startDate: '2021-03-01',
    endDate: '2021-06-30',
    location: 'Gujarat, India',
    roleType: 'Internship',
    technologies: [
      'Apache Kafka',
      'Apache Flink',
      'Apache Spark',
      'BioBERT',
      'Django',
      'MongoDB',
      'Python',
    ],
    highlights: [
      'Enabled fraud detection on 10M+ transactions daily',
      'Boosted ETL performance by 35%',
    ],
    description: [
      'Developed a real-time ingestion pipeline using Kafka and Flink for <b>fraud detection across 10M+ daily transactions</b>.',
      'Launched Django web app for secure healthcare report storage, <i>improving access latency by 40%</i>.',
      'Built a custom Spark optimizer to adjust shuffle partitions, <b>improving ETL performance by 35%</b>.',
      'Engineered a BioBERT + Regex-powered parsing bot for extracting structured data from clinical reports.',
    ],
  },
]

export default items
